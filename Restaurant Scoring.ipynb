{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 4: expected 5 fields, saw 6\\nSkipping line 5: expected 5 fields, saw 6\\nSkipping line 6: expected 5 fields, saw 6\\nSkipping line 11: expected 5 fields, saw 6\\nSkipping line 12: expected 5 fields, saw 6\\nSkipping line 26: expected 5 fields, saw 6\\nSkipping line 29: expected 5 fields, saw 6\\nSkipping line 37: expected 5 fields, saw 6\\nSkipping line 46: expected 5 fields, saw 6\\nSkipping line 55: expected 5 fields, saw 6\\nSkipping line 66: expected 5 fields, saw 6\\nSkipping line 68: expected 5 fields, saw 6\\nSkipping line 77: expected 5 fields, saw 6\\nSkipping line 88: expected 5 fields, saw 6\\nSkipping line 96: expected 5 fields, saw 6\\nSkipping line 113: expected 5 fields, saw 6\\nSkipping line 114: expected 5 fields, saw 6\\nSkipping line 115: expected 5 fields, saw 6\\nSkipping line 116: expected 5 fields, saw 6\\nSkipping line 121: expected 5 fields, saw 6\\nSkipping line 124: expected 5 fields, saw 6\\nSkipping line 143: expected 5 fields, saw 6\\nSkipping line 153: expected 5 fields, saw 6\\nSkipping line 154: expected 5 fields, saw 6\\nSkipping line 163: expected 5 fields, saw 6\\nSkipping line 165: expected 5 fields, saw 6\\nSkipping line 174: expected 5 fields, saw 6\\nSkipping line 177: expected 5 fields, saw 6\\nSkipping line 183: expected 5 fields, saw 6\\nSkipping line 184: expected 5 fields, saw 6\\nSkipping line 185: expected 5 fields, saw 6\\nSkipping line 186: expected 5 fields, saw 6\\nSkipping line 198: expected 5 fields, saw 6\\nSkipping line 219: expected 5 fields, saw 6\\nSkipping line 230: expected 5 fields, saw 6\\nSkipping line 239: expected 5 fields, saw 6\\nSkipping line 246: expected 5 fields, saw 6\\nSkipping line 248: expected 5 fields, saw 6\\nSkipping line 253: expected 5 fields, saw 6\\nSkipping line 260: expected 5 fields, saw 6\\nSkipping line 264: expected 5 fields, saw 6\\nSkipping line 267: expected 5 fields, saw 6\\nSkipping line 270: expected 5 fields, saw 6\\n'\n",
      "b'Skipping line 12: expected 5 fields, saw 6\\nSkipping line 15: expected 5 fields, saw 6\\nSkipping line 20: expected 5 fields, saw 6\\nSkipping line 23: expected 5 fields, saw 6\\nSkipping line 25: expected 5 fields, saw 6\\nSkipping line 26: expected 5 fields, saw 6\\nSkipping line 28: expected 5 fields, saw 6\\nSkipping line 32: expected 5 fields, saw 6\\nSkipping line 48: expected 5 fields, saw 6\\nSkipping line 50: expected 5 fields, saw 6\\nSkipping line 52: expected 5 fields, saw 6\\nSkipping line 58: expected 5 fields, saw 6\\nSkipping line 59: expected 5 fields, saw 6\\nSkipping line 61: expected 5 fields, saw 6\\nSkipping line 66: expected 5 fields, saw 6\\nSkipping line 68: expected 5 fields, saw 6\\nSkipping line 70: expected 5 fields, saw 6\\nSkipping line 78: expected 5 fields, saw 6\\nSkipping line 97: expected 5 fields, saw 6\\nSkipping line 99: expected 5 fields, saw 6\\nSkipping line 101: expected 5 fields, saw 6\\nSkipping line 105: expected 5 fields, saw 6\\nSkipping line 112: expected 5 fields, saw 6\\nSkipping line 115: expected 5 fields, saw 6\\nSkipping line 125: expected 5 fields, saw 6\\nSkipping line 132: expected 5 fields, saw 6\\nSkipping line 134: expected 5 fields, saw 6\\nSkipping line 135: expected 5 fields, saw 6\\nSkipping line 139: expected 5 fields, saw 6\\nSkipping line 144: expected 5 fields, saw 6\\nSkipping line 147: expected 5 fields, saw 6\\nSkipping line 162: expected 5 fields, saw 6\\nSkipping line 163: expected 5 fields, saw 6\\nSkipping line 168: expected 5 fields, saw 6\\nSkipping line 173: expected 5 fields, saw 6\\nSkipping line 177: expected 5 fields, saw 6\\nSkipping line 181: expected 5 fields, saw 6\\nSkipping line 184: expected 5 fields, saw 6\\nSkipping line 189: expected 5 fields, saw 6\\nSkipping line 192: expected 5 fields, saw 6\\nSkipping line 195: expected 5 fields, saw 6\\nSkipping line 197: expected 5 fields, saw 6\\nSkipping line 199: expected 5 fields, saw 6\\nSkipping line 200: expected 5 fields, saw 6\\nSkipping line 202: expected 5 fields, saw 6\\nSkipping line 203: expected 5 fields, saw 6\\nSkipping line 205: expected 5 fields, saw 6\\nSkipping line 207: expected 5 fields, saw 6\\nSkipping line 210: expected 5 fields, saw 6\\nSkipping line 211: expected 5 fields, saw 6\\nSkipping line 212: expected 5 fields, saw 6\\nSkipping line 213: expected 5 fields, saw 6\\nSkipping line 217: expected 5 fields, saw 6\\nSkipping line 222: expected 5 fields, saw 6\\nSkipping line 225: expected 5 fields, saw 6\\nSkipping line 229: expected 5 fields, saw 6\\nSkipping line 240: expected 5 fields, saw 6\\nSkipping line 242: expected 5 fields, saw 6\\nSkipping line 245: expected 5 fields, saw 6\\nSkipping line 266: expected 5 fields, saw 6\\nSkipping line 284: expected 5 fields, saw 6\\nSkipping line 293: expected 5 fields, saw 6\\nSkipping line 310: expected 5 fields, saw 6\\nSkipping line 317: expected 5 fields, saw 6\\nSkipping line 326: expected 5 fields, saw 6\\nSkipping line 328: expected 5 fields, saw 6\\nSkipping line 350: expected 5 fields, saw 6\\nSkipping line 366: expected 5 fields, saw 6\\nSkipping line 369: expected 5 fields, saw 6\\nSkipping line 375: expected 5 fields, saw 6\\nSkipping line 379: expected 5 fields, saw 6\\nSkipping line 387: expected 5 fields, saw 6\\nSkipping line 389: expected 5 fields, saw 6\\nSkipping line 405: expected 5 fields, saw 6\\nSkipping line 413: expected 5 fields, saw 6\\nSkipping line 417: expected 5 fields, saw 6\\nSkipping line 418: expected 5 fields, saw 6\\nSkipping line 420: expected 5 fields, saw 6\\nSkipping line 421: expected 5 fields, saw 6\\nSkipping line 423: expected 5 fields, saw 6\\nSkipping line 428: expected 5 fields, saw 6\\nSkipping line 436: expected 5 fields, saw 6\\nSkipping line 438: expected 5 fields, saw 6\\nSkipping line 444: expected 5 fields, saw 6\\nSkipping line 447: expected 5 fields, saw 6\\nSkipping line 448: expected 5 fields, saw 6\\nSkipping line 449: expected 5 fields, saw 6\\nSkipping line 451: expected 5 fields, saw 6\\nSkipping line 453: expected 5 fields, saw 6\\nSkipping line 454: expected 5 fields, saw 6\\nSkipping line 465: expected 5 fields, saw 6\\nSkipping line 466: expected 5 fields, saw 6\\nSkipping line 467: expected 5 fields, saw 6\\nSkipping line 473: expected 5 fields, saw 6\\nSkipping line 474: expected 5 fields, saw 6\\nSkipping line 482: expected 5 fields, saw 6\\nSkipping line 488: expected 5 fields, saw 6\\nSkipping line 493: expected 5 fields, saw 6\\nSkipping line 495: expected 5 fields, saw 6\\nSkipping line 508: expected 5 fields, saw 6\\nSkipping line 512: expected 5 fields, saw 6\\nSkipping line 535: expected 5 fields, saw 6\\n'\n",
      "b'Skipping line 5: expected 5 fields, saw 6\\nSkipping line 6: expected 5 fields, saw 6\\nSkipping line 15: expected 5 fields, saw 6\\nSkipping line 17: expected 5 fields, saw 6\\nSkipping line 26: expected 5 fields, saw 6\\nSkipping line 33: expected 5 fields, saw 6\\nSkipping line 39: expected 5 fields, saw 6\\nSkipping line 45: expected 5 fields, saw 6\\nSkipping line 52: expected 5 fields, saw 6\\nSkipping line 53: expected 5 fields, saw 6\\nSkipping line 55: expected 5 fields, saw 6\\nSkipping line 59: expected 5 fields, saw 6\\nSkipping line 60: expected 5 fields, saw 6\\nSkipping line 65: expected 5 fields, saw 6\\nSkipping line 66: expected 5 fields, saw 6\\nSkipping line 72: expected 5 fields, saw 6\\nSkipping line 73: expected 5 fields, saw 6\\nSkipping line 78: expected 5 fields, saw 6\\nSkipping line 84: expected 5 fields, saw 6\\nSkipping line 87: expected 5 fields, saw 6\\nSkipping line 89: expected 5 fields, saw 6\\nSkipping line 90: expected 5 fields, saw 6\\nSkipping line 92: expected 5 fields, saw 6\\nSkipping line 94: expected 5 fields, saw 6\\nSkipping line 96: expected 5 fields, saw 6\\nSkipping line 103: expected 5 fields, saw 6\\nSkipping line 106: expected 5 fields, saw 6\\nSkipping line 109: expected 5 fields, saw 6\\nSkipping line 120: expected 5 fields, saw 6\\nSkipping line 125: expected 5 fields, saw 6\\nSkipping line 127: expected 5 fields, saw 6\\nSkipping line 130: expected 5 fields, saw 6\\nSkipping line 136: expected 5 fields, saw 6\\nSkipping line 138: expected 5 fields, saw 6\\nSkipping line 143: expected 5 fields, saw 6\\nSkipping line 144: expected 5 fields, saw 6\\nSkipping line 159: expected 5 fields, saw 6\\nSkipping line 163: expected 5 fields, saw 6\\nSkipping line 164: expected 5 fields, saw 6\\nSkipping line 169: expected 5 fields, saw 6\\nSkipping line 173: expected 5 fields, saw 6\\nSkipping line 175: expected 5 fields, saw 6\\nSkipping line 178: expected 5 fields, saw 6\\nSkipping line 187: expected 5 fields, saw 6\\nSkipping line 188: expected 5 fields, saw 6\\nSkipping line 190: expected 5 fields, saw 6\\nSkipping line 194: expected 5 fields, saw 6\\nSkipping line 199: expected 5 fields, saw 6\\nSkipping line 205: expected 5 fields, saw 6\\nSkipping line 217: expected 5 fields, saw 6\\nSkipping line 219: expected 5 fields, saw 6\\nSkipping line 221: expected 5 fields, saw 6\\nSkipping line 225: expected 5 fields, saw 6\\nSkipping line 227: expected 5 fields, saw 6\\n'\n"
     ]
    }
   ],
   "source": [
    "#Merging all restaurant data \n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# get data file names\n",
    "path =r\"C:/Users/YESIM/Desktop/TEXT MINING PROJECT/AMSTERDAM/RESTAURANTS/All\"\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    dfs.append(pd.read_csv(filename, error_bad_lines=False) )\n",
    "\n",
    "# Concatenate all data into one DataFrame\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data Preparation\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "df = df.fillna(value=str(0))\n",
    "\n",
    "df[\"full_text\"] = df[\"title\"] + \". \" + df[\"review\"]\n",
    "\n",
    "#Reviews with 3 stars or lower are negative sentiment and others are positive sentiment \n",
    "def get_class(stars):\n",
    "    score = int(stars[0])\n",
    "    if score > 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def review_count(review_count):\n",
    "    return review_count[0]\n",
    "\n",
    "def help_count(help_count):\n",
    "    return help_count[0]\n",
    "\n",
    "def del_more(full_text):\n",
    "    if full_text[-1] == \"e\" and full_text[-2] == \"r\" and full_text[-3] == \"o\" and full_text[-4] == \"M\":\n",
    "        return full_text[0:-8]\n",
    "    else:\n",
    "        return full_text\n",
    "    \n",
    "df[\"review count\"] = df[\"review_count\"].apply(review_count).astype(int)\n",
    "df[\"help count\"] = df[\"help_count\"].apply(help_count).astype(int)\n",
    "df[\"sentiment\"] = df[\"stars\"].apply(get_class)\n",
    "df[\"id\"] = range(0,len(df))\n",
    "df[\"full_text\"] = df[\"full_text\"].apply(del_more)\n",
    "\n",
    "df_sent = df[[\"full_text\", \"sentiment\"]]\n",
    "\n",
    "df_sent[\"full_text\"]\n",
    "\n",
    "#Create equal samples of negative and positive reviews, 2100 for each\n",
    "\n",
    "df_sent_0 = df_sent[df_sent[\"sentiment\"]==0]\n",
    "df_sent_1 = df_sent[df_sent[\"sentiment\"]==1]\n",
    "df_sent_1 = df_sent_1.sample(n=2100)\n",
    "df_sent = pd.concat([df_sent_0, df_sent_1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PREPROCESS ARGUMENT OF CLASS NONE OR FUNCTION\n",
    "\n",
    "#Preprocess, Tokenize and Remove Duplicates\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ''.join(emoticons).replace('-', '')    \n",
    "    text = re.sub(\"[',.,!,?,&,%,$,@,(,)]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    text = [word for word in text.split() if word not in stop]\n",
    "    text = list(set(text))\n",
    "    return text\n",
    "\n",
    "df_sent[\"full_text\"] = df[\"full_text\"].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split data into train and test\n",
    "\n",
    "X = df_sent[\"full_text\"].values\n",
    "y = df_sent[\"sentiment\"].values\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=0.2 ,random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FREQ METHODS OF CLASS -- WHAT TO DO FOR SAMPLES WITH MORE CLASS ?\n",
    "\n",
    "# Training set consisting individual reviews and classes\n",
    "X_train\n",
    "y_train\n",
    "\n",
    "\n",
    "#Counting distinct words in both classes\n",
    "X_train[y_train==0]\n",
    "X_train[y_train==1]\n",
    "\n",
    "X_train_0 = []\n",
    "X_train_1 = []\n",
    "\n",
    "for lists in X_train[y_train==0]:\n",
    "    for words in lists:\n",
    "        X_train_0.append(words)\n",
    "for lists in X_train[y_train==1]:\n",
    "    for words in lists:\n",
    "        X_train_1.append(words)    \n",
    "        \n",
    "from collections import Counter\n",
    "counts_0 = Counter(X_train_0)\n",
    "counts_1 = Counter(X_train_1)\n",
    "\n",
    "#Calculate P(C) for C = 0,1\n",
    "class0_prob = len(X_train_0)/len(X_train_0+X_train_1)\n",
    "class1_prob = len(X_train_1)/len(X_train_0+X_train_1)\n",
    "\n",
    "\n",
    "#Calculate class freq of each word f(W,C)\n",
    "freq_0_list = []\n",
    "freq_1_list = []\n",
    "\n",
    "for key in counts_0:\n",
    "    array = []\n",
    "    array.append(key)\n",
    "    array.append(counts_0[key]/len(X_train_0))\n",
    "    freq_0_list.append(array)\n",
    "\n",
    "for key in counts_1:\n",
    "    array = []\n",
    "    array.append(key)\n",
    "    array.append(counts_1[key]/len(X_train_1))\n",
    "    freq_1_list.append(array)\n",
    "\n",
    "#Calculate total freq of each words sum(f(W,Ci)) for i=0,1\n",
    "#FREQ TOTAL I TEKRAR HESAPLAMAK LAZIM\n",
    "\n",
    "all_counts = Counter(counts_0 + counts_1)\n",
    "\n",
    "freq_total_list = []\n",
    "\n",
    "for key in all_counts:\n",
    "    array = []\n",
    "    array.append(key)\n",
    "    array.append( (counts_0[key]/len(X_train_0)) + (counts_1[key]/len(X_train_1)))\n",
    "    freq_total_list.append(array)    \n",
    "\n",
    "\n",
    "minfreq1 = 0.000000000000001\n",
    "minfreq0 = 0.000000000000001\n",
    "\n",
    "a = 5\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YESIM\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\YESIM\\Desktop\\TEXT MINING PROJECT\\AMSTERDAM\\RESTAURANTS\\All\\Casa Nostra.csv\"\n",
    "\n",
    "df_2 = pd.read_csv(path, error_bad_lines=False, index_col = None)\n",
    "\n",
    "\n",
    "#Data Preparation\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "df = df_2.fillna(value=str(0))\n",
    "\n",
    "df[\"full_text\"] = df[\"title\"] + \". \" + df[\"review\"]\n",
    "\n",
    "#Reviews with 3 stars or lower are negative sentiment and others are positive sentiment \n",
    "def get_class(stars):\n",
    "    score = int(stars[0])\n",
    "    if score > 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def review_count(review_count):\n",
    "    return review_count[0]\n",
    "\n",
    "def help_count(help_count):\n",
    "    return help_count[0]\n",
    "\n",
    "def del_more(full_text):\n",
    "    if full_text[-1] == \"e\" and full_text[-2] == \"r\" and full_text[-3] == \"o\" and full_text[-4] == \"M\":\n",
    "        return full_text[0:-8]\n",
    "    else:\n",
    "        return full_text\n",
    "    \n",
    "df[\"review count\"] = df[\"review_count\"].apply(review_count).astype(int)\n",
    "df[\"help count\"] = df[\"help_count\"].apply(help_count).astype(int)\n",
    "df[\"sentiment\"] = df[\"stars\"].apply(get_class)\n",
    "df[\"id\"] = range(0,len(df))\n",
    "df[\"full_text\"] = df[\"full_text\"].apply(del_more)\n",
    "\n",
    "df_sent = df[[\"full_text\", \"sentiment\",\"review_count\", \"help_count\"]]\n",
    "\n",
    "df_sent[\"full_text\"]\n",
    "\n",
    "#PREPROCESS ARGUMENT OF CLASS NONE OR FUNCTION\n",
    "\n",
    "#Preprocess, Tokenize and Remove Duplicates\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ''.join(emoticons).replace('-', '')    \n",
    "    text = re.sub(\"[',.,!,?,&,%,$,@,(,)]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    text = [word for word in text.split() if word not in stop]\n",
    "    text = list(set(text))\n",
    "    return text\n",
    "\n",
    "df_sent[\"full_text\"] = df[\"full_text\"].apply(preprocess)\n",
    "\n",
    "X_Rest = df_sent[\"full_text\"].values\n",
    "y_Rest = df_sent[\"sentiment\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction Method of CLASS\n",
    "\n",
    "pred = []\n",
    "#for array in X_test:\n",
    "for array in X_Rest:\n",
    "\n",
    "    freq1 = []\n",
    "    freq0 = []\n",
    "    freq_total = []\n",
    "\n",
    "    for word in array:   \n",
    "\n",
    "        count = 0\n",
    "        for lists in freq_1_list:\n",
    "            if lists[0] == word:\n",
    "                freq1.append(lists[1])\n",
    "                break\n",
    "            else:\n",
    "                count = count + 1\n",
    "            if count == len(freq_1_list):\n",
    "                freq1.append(0)\n",
    "\n",
    "\n",
    "\n",
    "    for word in array:   \n",
    "\n",
    "        count = 0\n",
    "        for lists in freq_0_list:\n",
    "            if lists[0] == word:\n",
    "                freq0.append(lists[1])\n",
    "                break\n",
    "            else:\n",
    "                count = count + 1\n",
    "            if count == len(freq_0_list):\n",
    "                freq0.append(0)            \n",
    "\n",
    "\n",
    "    for word in array:   \n",
    "\n",
    "        count = 0\n",
    "        for lists in freq_total_list:\n",
    "            if lists[0] == word:\n",
    "                freq_total.append(lists[1])\n",
    "                break\n",
    "            else:\n",
    "                count = count + 1\n",
    "            if count == len(freq_total_list):\n",
    "                freq_total.append(0)  \n",
    "\n",
    "\n",
    "    freq_total_1 = list(freq_total)\n",
    "    freq_total_0 = list(freq_total)\n",
    "\n",
    "    #Smoothing the frequencies\n",
    "    for i in range(0,len(freq1)):\n",
    "\n",
    "        if freq1[i] == 0:\n",
    "            if freq_total_1[i] == 0:\n",
    "                freq1[i] = 0.5\n",
    "                freq_total_1[i] = 1\n",
    "            else:\n",
    "                freq1[i] = minfreq1\n",
    "\n",
    "    for i in range(0,len(freq0)):\n",
    "\n",
    "        if freq0[i] == 0:\n",
    "            if freq_total_0[i] == 0:\n",
    "                freq0[i] = 0.5\n",
    "                freq_total_0[i] = 1\n",
    "            else:\n",
    "                freq0[i] = minfreq1\n",
    "\n",
    "\n",
    "    score0 = np.sum(np.log10(np.array(freq0)/np.array(freq_total_0))) + np.log10(class0_prob)\n",
    "    score1 = np.sum(np.log10(np.array(freq1)/np.array(freq_total_1))) + np.log10(class1_prob)\n",
    "\n",
    "    if score0 > score1:\n",
    "        pred.append(0)\n",
    "    else:\n",
    "        pred.append(1)         \n",
    "\n",
    "    freq1 = []\n",
    "    freq0 = []\n",
    "    freq_total = []\n",
    "    \n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81818181818181823"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pred)/len(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
