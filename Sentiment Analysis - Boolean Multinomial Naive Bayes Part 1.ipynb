{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merging all restaurant data \n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# get data file names\n",
    "path =r\"C:/Users/YESIM/Desktop/TEXT MINING PROJECT/AMSTERDAM/RESTAURANTS/All\"\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    dfs.append(pd.read_csv(filename, error_bad_lines=False) )\n",
    "\n",
    "# Concatenate all data into one DataFrame\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data Preparation\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "df = df.fillna(value=str(0))\n",
    "\n",
    "df[\"full_text\"] = df[\"title\"] + \". \" + df[\"review\"]\n",
    "\n",
    "#Reviews with 3 stars or lower are negative sentiment and others are positive sentiment \n",
    "def get_class(stars):\n",
    "    score = int(stars[0])\n",
    "    if score > 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def review_count(review_count):\n",
    "    return review_count[0]\n",
    "\n",
    "def help_count(help_count):\n",
    "    return help_count[0]\n",
    "\n",
    "def del_more(full_text):\n",
    "    if full_text[-1] == \"e\" and full_text[-2] == \"r\" and full_text[-3] == \"o\" and full_text[-4] == \"M\":\n",
    "        return full_text[0:-8]\n",
    "    else:\n",
    "        return full_text\n",
    "    \n",
    "df[\"review count\"] = df[\"review_count\"].apply(review_count).astype(int)\n",
    "df[\"help count\"] = df[\"help_count\"].apply(help_count).astype(int)\n",
    "df[\"sentiment\"] = df[\"stars\"].apply(get_class)\n",
    "df[\"id\"] = range(0,len(df))\n",
    "df[\"full_text\"] = df[\"full_text\"].apply(del_more)\n",
    "\n",
    "df_sent = df[[\"full_text\", \"sentiment\"]]\n",
    "\n",
    "df_sent[\"full_text\"]\n",
    "\n",
    "#Create equal samples of negative and positive reviews, 2100 for each\n",
    "\n",
    "df_sent_0 = df_sent[df_sent[\"sentiment\"]==0]\n",
    "df_sent_1 = df_sent[df_sent[\"sentiment\"]==1]\n",
    "df_sent_1 = df_sent_1.sample(n=2100)\n",
    "df_sent = pd.concat([df_sent_0, df_sent_1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PREPROCESS ARGUMENT OF CLASS NONE OR FUNCTION\n",
    "\n",
    "#Preprocess, Tokenize and Remove Duplicates\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ''.join(emoticons).replace('-', '')    \n",
    "    text = re.sub(\"[',.,!,?,&,%,$,@,(,)]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    text = [word for word in text.split() if word not in stop]\n",
    "    text = list(set(text))\n",
    "    return text\n",
    "\n",
    "df_sent[\"full_text\"] = df[\"full_text\"].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split data into train and test\n",
    "\n",
    "X = df_sent[\"full_text\"].values\n",
    "y = df_sent[\"sentiment\"].values\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=0.2 ,random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CROSS VALIDATION ARGUMENT OF CLASS NONE OR NUMBER OF FOLDS\n",
    "\n",
    "#Stratified kfold k=5\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "skf = StratifiedKFold(y, n_folds=5)\n",
    "\n",
    "train_index = []\n",
    "val_index = []\n",
    "\n",
    "for train_ind, val_ind in skf:\n",
    "    train_index.append(train_ind)\n",
    "    val_index.append(val_ind)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FREQ METHODS OF CLASS -- WHAT TO DO FOR SAMPLES WITH MORE CLASS ?\n",
    "\n",
    "# Training set consisting individual reviews and classes\n",
    "X_train\n",
    "y_train\n",
    "\n",
    "\n",
    "#Counting distinct words in both classes\n",
    "X_train[y_train==0]\n",
    "X_train[y_train==1]\n",
    "\n",
    "X_train_0 = []\n",
    "X_train_1 = []\n",
    "\n",
    "for lists in X_train[y_train==0]:\n",
    "    for words in lists:\n",
    "        X_train_0.append(words)\n",
    "for lists in X_train[y_train==1]:\n",
    "    for words in lists:\n",
    "        X_train_1.append(words)    \n",
    "        \n",
    "from collections import Counter\n",
    "counts_0 = Counter(X_train_0)\n",
    "counts_1 = Counter(X_train_1)\n",
    "\n",
    "#Calculate P(C) for C = 0,1\n",
    "class0_prob = len(X_train_0)/len(X_train_0+X_train_1)\n",
    "class1_prob = len(X_train_1)/len(X_train_0+X_train_1)\n",
    "\n",
    "\n",
    "#Calculate class freq of each word f(W,C)\n",
    "freq_0_list = []\n",
    "freq_1_list = []\n",
    "\n",
    "for key in counts_0:\n",
    "    array = []\n",
    "    array.append(key)\n",
    "    array.append(counts_0[key]/len(X_train_0))\n",
    "    freq_0_list.append(array)\n",
    "\n",
    "for key in counts_1:\n",
    "    array = []\n",
    "    array.append(key)\n",
    "    array.append(counts_1[key]/len(X_train_1))\n",
    "    freq_1_list.append(array)\n",
    "\n",
    "#Calculate total freq of each words sum(f(W,Ci)) for i=0,1\n",
    "#FREQ TOTAL I TEKRAR HESAPLAMAK LAZIM\n",
    "\n",
    "all_counts = Counter(counts_0 + counts_1)\n",
    "\n",
    "freq_total_list = []\n",
    "\n",
    "for key in all_counts:\n",
    "    array = []\n",
    "    array.append(key)\n",
    "    array.append(((counts_0[key]/len(X_train_0) + counts_1[key]/len(X_train_1)))\n",
    "    freq_total_list.append(array)\n",
    "\n",
    "\n",
    "minfreq1 = 0.000000000000001\n",
    "minfreq0 = 0.000000000000001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prediction Method of CLASS\n",
    "\n",
    "pred = []\n",
    "#for array in X_test:\n",
    "for array in X_train:\n",
    "\n",
    "    freq1 = []\n",
    "    freq0 = []\n",
    "    freq_total = []\n",
    "\n",
    "    for word in array:   \n",
    "\n",
    "        count = 0\n",
    "        for lists in freq_1_list:\n",
    "            if lists[0] == word:\n",
    "                freq1.append(lists[1])\n",
    "                break\n",
    "            else:\n",
    "                count = count + 1\n",
    "            if count == len(freq_1_list):\n",
    "                freq1.append(0)\n",
    "\n",
    "\n",
    "\n",
    "    for word in array:   \n",
    "\n",
    "        count = 0\n",
    "        for lists in freq_0_list:\n",
    "            if lists[0] == word:\n",
    "                freq0.append(lists[1])\n",
    "                break\n",
    "            else:\n",
    "                count = count + 1\n",
    "            if count == len(freq_0_list):\n",
    "                freq0.append(0)            \n",
    "\n",
    "\n",
    "    for word in array:   \n",
    "\n",
    "        count = 0\n",
    "        for lists in freq_total_list:\n",
    "            if lists[0] == word:\n",
    "                freq_total.append(lists[1])\n",
    "                break\n",
    "            else:\n",
    "                count = count + 1\n",
    "            if count == len(freq_total_list):\n",
    "                freq_total.append(0)  \n",
    "\n",
    "\n",
    "    freq_total_1 = list(freq_total)\n",
    "    freq_total_0 = list(freq_total)\n",
    "\n",
    "    #Smoothing the frequencies\n",
    "    for i in range(0,len(freq1)):\n",
    "\n",
    "        if freq1[i] == 0:\n",
    "            if freq_total_1[i] == 0:\n",
    "                freq1[i] = 0.5\n",
    "                freq_total_1[i] = 1\n",
    "            else:\n",
    "                freq1[i] = minfreq1\n",
    "\n",
    "    for i in range(0,len(freq0)):\n",
    "\n",
    "        if freq0[i] == 0:\n",
    "            if freq_total_0[i] == 0:\n",
    "                freq0[i] = 0.5\n",
    "                freq_total_0[i] = 1\n",
    "            else:\n",
    "                freq0[i] = minfreq1\n",
    "\n",
    "\n",
    "    score0 = np.sum(np.log10(np.array(freq0)/np.array(freq_total_0))) + np.log10(class0_prob)\n",
    "    score1 = np.sum(np.log10(np.array(freq1)/np.array(freq_total_1))) + np.log10(class1_prob)\n",
    "\n",
    "    if score0 > score1:\n",
    "        pred.append(0)\n",
    "    else:\n",
    "        pred.append(1)         \n",
    "\n",
    "    freq1 = []\n",
    "    freq0 = []\n",
    "    freq_total = []\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Score Method of CLASS\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cmat = confusion_matrix(y_test, pred)\n",
    "\n",
    "TN_RATIO = cmat[1][1]/np.sum(cmat[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f1_score = f1_score(y_test, pred, average = \"micro\")\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "\n",
    "print(\"Accuracy: %f\\nPrecision: %f\\nRecall: %f\\nF1 score(micro): %f\" %(accuracy, precision ,recall, f1_score))\n",
    "print(\"TN/TN+FN: %f\" %TN_RATIO )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(np.array([2,4,4]) == np.array([2,4,3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
